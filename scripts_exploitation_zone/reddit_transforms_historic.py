"""
For this code to work, we need python 3.11.8, PySpark 3.5.* Delta Lake 2.4.*
Refer to: https://stackoverflow.com/questions/77369508/python-worker-keeps-on-crashing-in-pyspark

To tweak the Spark configuration:
  If you don't have conda, get it from here https://repo.anaconda.com/miniconda/
    version: Miniconda3-py311_*
  conda create --prefix D:/python_envs/py3118 python=3.11.8 -y
  conda activate D:/python_envs/py3118
  pip install pyspark==3.5.* delta-spark==3.3.* numpy

Run the code when this is done
"""

import pyspark
from delta import configure_spark_with_delta_pip
from pyspark.sql.functions import (
    col, when, length, size, udf, expr, lit
)
from pyspark.sql.types import ArrayType, StringType
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    RegexTokenizer, StopWordsRemover,
    NGram, SQLTransformer,
    CountVectorizer, IDF
)
from pyspark.ml.linalg import SparseVector

SERVICE_ACCOUNT_KEY = "C:\\Users\\Kristof\\.gcs\\calm-depot-454710-p0-4cf918c71f69.json"
TRUSTED_DELTA_PATH  = "gs://group-1-delta-lake-lets-talk/trusted_zone/reddit"
EXPLOIT_DELTA_PATH      = "gs://group-1-delta-lake-lets-talk/exploitation_zone/reddit"
TOP_N                   = 3

# map subreddit to a main topic
TOPIC_MAP = {
    "AskReddit":           "General Chat",
    "NoStupidQuestions":   "Advice & Life",
    "ExplainLikeImFive":   "Science",
    "askscience":          "Science",
    "AskHistorians":       "History",
    "AskMen":              "Social & Culture",
    "AskWomen":            "Social & Culture",
    "Ask_Politics":        "Social & Culture",
    "AskScienceFiction":   "Entertainment & Fiction",
    "AskEngineers":        "Engineering",
    "DoesAnybodyElse":     "General Chat",
    "questions":           "General Chat",
    "ask":                 "General Chat",
    "TrueAskReddit":       "General Chat"
}

def spark_configure(app_name: str):
    conf = (
        pyspark.conf.SparkConf()
          .setAppName(app_name)
          .setMaster("local[*]")
          .set("spark.sql.catalog.spark_catalog",     "org.apache.spark.sql.delta.catalog.DeltaCatalog")
          .set("spark.sql.extensions",                "io.delta.sql.DeltaSparkSessionExtension")
          .set("spark.hadoop.fs.gs.impl",             "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
          .set("spark.hadoop.google.cloud.auth.service.account.enable", "true")
          .set("spark.hadoop.google.cloud.auth.service.account.json.keyfile", SERVICE_ACCOUNT_KEY)
          .set("spark.hadoop.fs.AbstractFileSystem.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
          .set("spark.jars",
               "https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar")
          .set("spark.sql.shuffle.partitions", "4")
    )
    builder = pyspark.sql.SparkSession.builder.config(conf=conf)
    return configure_spark_with_delta_pip(builder).getOrCreate()

def extract_and_exploit(top_n=TOP_N):
    spark = spark_configure("reddit_exploitation")
    # read trusted zone
    df = spark.read.format("delta").load(TRUSTED_DELTA_PATH)

    #################################### KEYWORD EXTRACTION START ##############################################
    # fallback to title if selftext is empty
    df = df.withColumn(
        "text",
        when(length(col("selftext_clean")) > 0, col("selftext_clean"))
        .otherwise(col("title_clean"))
    )

    # ML pipeline
    tokenizer = RegexTokenizer(inputCol="text", outputCol="tokens_raw", pattern="\\W+")
    # remove stop words
    extra_sw = ["reddit","ask","question","thanks","please"]
    remover = StopWordsRemover(inputCol="tokens_raw", outputCol="tokens",
                                  stopWords=StopWordsRemover.loadDefaultStopWords("english") + extra_sw)
    # 2-3 word n-grams
    bigram = NGram(n=2, inputCol="tokens", outputCol="bigrams")
    trigram = NGram(n=3, inputCol="tokens", outputCol="trigrams")
    assembler = SQLTransformer(statement="""
        SELECT *, array_union(tokens, array_union(bigrams, trigrams)) AS all_grams
        FROM __THIS__
    """)
    cv = CountVectorizer(inputCol="all_grams", outputCol="rawFeatures",
                                  vocabSize=10_000, minDF=3)
    idf = IDF(inputCol="rawFeatures", outputCol="features")

    pipeline = Pipeline(stages=[tokenizer, remover, bigram, trigram, assembler, cv, idf])
    model = pipeline.fit(df)
    df_feats = model.transform(df)

    # broadcast the vocabulary
    vocab = model.stages[-2].vocabulary
    bc_vocab = spark.sparkContext.broadcast(vocab)

    # Python UDF that only looks at the sparse vector's indices and values
    def top_terms(sv):
        if not isinstance(sv, SparseVector):
            return []
        idx_vals = list(zip(sv.indices, sv.values))
        # sort desc by weight
        top = sorted(idx_vals, key=lambda x: -x[1])[:top_n]
        return [bc_vocab.value[i] for i, w in top if w > 0]

    top_udf = udf(top_terms, ArrayType(StringType()))

    df_kw = df_feats.withColumn("keywords", top_udf(col("features")))

    # post-filtering to drop short, numeric or generic things, this can be improved,
    # but for the proof of concept, this is good enough
    df_kw = df_kw.withColumn(
        "keywords",
        expr(
            """
            filter(
            keywords,
            x ->
                length(x) > 3
                AND NOT x RLIKE '^[0-9]+$'
                AND NOT x RLIKE '\\\\b(oh|much|want|one|thing|dont|find|please|thanks)\\\\b'
            )
            """
        )
    ).filter(size(col("keywords")) > 0) # remove empty keywords

    ############################## KEYWORD EXTRACTION END ##############################################

    # map subreddit to: topic, add source
    bc_topic_map = spark.sparkContext.broadcast(TOPIC_MAP)
    topic_udf = udf(lambda s: bc_topic_map.value.get(s, "Other"), StringType())

    df_out = (
      df_kw
        .withColumn("source", lit("reddit"))
        .withColumn("topic",  topic_udf(col("subreddit")))
        .select(
            "source",
            "subreddit",
            "topic",
            "title",
            "keywords",
            col("score").alias("upvotes"),
            "num_comments",
            "created_utc_ts"
        )
    )

    df_out.write \
        .format("delta") \
        .mode("overwrite") \
        .save(EXPLOIT_DELTA_PATH)

    print("Expl. Delta table written to:", EXPLOIT_DELTA_PATH)
    spark.stop()

if __name__ == "__main__":
    extract_and_exploit()
