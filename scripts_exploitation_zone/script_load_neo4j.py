import pyspark
from delta import configure_spark_with_delta_pip
from pyspark.sql import functions as F

USER = "stef4"
NEO4J_URL = "neo4j+s://XXXXXXXXXXX.databases.neo4j.io"

NEO4J_USER = "neo4j"

NEO4J_PASSWORD = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX" 

def spark_configure(user, name_operation):
    conf = (
        pyspark.conf.SparkConf()
        .setAppName(name_operation)
        .set(
            "spark.sql.catalog.spark_catalog",
            "org.apache.spark.sql.delta.catalog.DeltaCatalog",
        )
        .set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .set("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
        .set("spark.files.cleanup", "false")
        .set("spark.hadoop.google.cloud.auth.service.account.enable", "true")
        .set("spark.hadoop.google.cloud.auth.service.account.json.keyfile",
        "C:\\Users\\" + user + "\\.gcs\\calm-depot-454710-p0-4cf918c71f69.json")
        .set("spark.sql.shuffle.partitions", "4")
        .set("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
        .set(
        "spark.jars",
        "./jars/neo4j-spark-connector-5.3.7-s_2.12.jar,https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar"
        )
        .setMaster("local[*]")
    )

    builder = pyspark.sql.SparkSession.builder.config(conf=conf)
    spark_session = (configure_spark_with_delta_pip(builder).getOrCreate())
    spark_session.conf.set("neo4j.url", NEO4J_URL)
    spark_session.conf.set("neo4j.authentication.basic.username", NEO4J_USER)
    spark_session.conf.set("neo4j.authentication.basic.password", NEO4J_PASSWORD)
    spark_session.conf.set("neo4j.database", "neo4j")
    spark_session.conf.set("neo4j.batch.size", "25000")
    return spark_session


def process_data():
    spark = spark_configure(user = USER, name_operation='load_graph_data')

    # Loading all the question files
    delta_files = ["gs://group-1-delta-lake-lets-talk/trusted_zone/trivia/",
                   "gs://group-1-delta-lake-lets-talk/trusted_zone/wikipedia/",
                   "gs://group-1-delta-lake-lets-talk/trusted_zone/stack_exchange/",
                   "gs://group-1-delta-lake-lets-talk/trusted_zone/reddit/"]
            
    for delta_file in delta_files:
            print("Loading file: " + delta_file)
            df = spark.read.format("delta").load(delta_file)

            # Filter only non-null questions with an id
            questions_df = df.filter(df["id"].isNotNull())#.limit(100)
            
            # Exclude column keyword and topic
            columns_to_select = [col for col in questions_df.columns if col not in ("keywords", "topic")]

            # Create Question nodes
            print("Creating Question nodes")
            questions_df.selectExpr(*columns_to_select)\
                .write.format("org.neo4j.spark.DataSource") \
                .mode("overwrite") \
                .option("labels", ":Question") \
                .option("node.keys", "id") \
                .save()

            # Explode keywords
            keywords_df = questions_df.selectExpr("id", "explode(keywords) as keyword")

            # Create Keyword nodes
            print("Creating Keyword nodes")
            keywords_df.selectExpr("keyword").distinct().write.format("org.neo4j.spark.DataSource") \
                .mode("overwrite") \
                .option("labels", ":Keyword") \
                .option("node.keys", "keyword") \
                .save()

            # Create HAS_KEYWORD relationships
            print("Creating HAS_KEYWORD relationships")
            # Use repartition to avoid deadlock error - issue only for stackexchange data
            keywords_df.repartition(1).write.format("org.neo4j.spark.DataSource") \
                .mode("overwrite") \
                .option("relationship", "HAS_KEYWORD") \
                .option("relationship.save.strategy", "keys") \
                .option("relationship.source.labels", ":Question") \
                .option("relationship.source.node.keys", "id:id") \
                .option("relationship.target.labels", ":Keyword") \
                .option("relationship.target.node.keys", "keyword:keyword") \
                .save()

            # Create Topic nodes
            print("Creating Topic nodes")
            topics_df = questions_df.selectExpr("topic").filter("topic is not null").distinct()
            topics_df.write.format("org.neo4j.spark.DataSource") \
                .mode("overwrite") \
                .option("labels", ":Topic") \
                .option("node.keys", "topic") \
                .save()

            # Create HAS_TOPIC relationship
            print("Creating HAS_TOPIC relationship")
            topic_rels_df = questions_df.selectExpr("id", "topic").filter("topic is not null")
            topic_rels_df.repartition(1).write.format("org.neo4j.spark.DataSource") \
                .mode("overwrite") \
                .option("relationship", "HAS_TOPIC") \
                .option("relationship.save.strategy", "keys") \
                .option("relationship.source.labels", ":Question") \
                .option("relationship.source.node.keys", "id:id") \
                .option("relationship.target.labels", ":Topic") \
                .option("relationship.target.node.keys", "topic:topic") \
                .save()
            
            # Create BELONGS relationship from Keyword to Topic
            print("Creating BELONGS relationship")
            belongs_df = questions_df \
                .selectExpr("explode(keywords) as keyword", "topic") \
                .filter("keyword is not null and topic is not null") \
                .distinct()

            belongs_df.repartition(1).write.format("org.neo4j.spark.DataSource") \
                .mode("overwrite") \
                .option("relationship", "BELONGS") \
                .option("relationship.save.strategy", "keys") \
                .option("relationship.source.labels", ":Keyword") \
                .option("relationship.source.node.keys", "keyword:keyword") \
                .option("relationship.target.labels", ":Topic") \
                .option("relationship.target.node.keys", "topic:topic") \
                .save()
            print("--------------------------------------")
        
    # Now we create all the users
    print("Loading Users")
    users_delta_path = "gs://group-1-delta-lake-lets-talk/trusted_zone/user_topic_keywords/"
    # Load the parquet (Delta Lake)
    users_df = spark.read.format("parquet").load(users_delta_path) \
        .filter("user_id is not null")#.limit(100)

    # Aggregate keywords and topics per user, keep name (assuming same for each user_id)
    agg_df = users_df.groupBy("user_id", "name") \
        .agg(
            F.collect_set("topic").alias("topics"),               # unique topics per user
            F.flatten(F.collect_set("keywords")).alias("keywords") # flatten keywords lists, then unique
        )

    # Create User nodes
    print("Creating User nodes")
    agg_df.selectExpr("user_id", "name") \
        .write.format("org.neo4j.spark.DataSource") \
        .mode("overwrite") \
        .option("labels", ":User") \
        .option("node.keys", "user_id") \
        .save()

    # Create Keyword nodes (unique keywords across all users)
    print("Creating Keyword nodes")
    user_keywords_df = agg_df.selectExpr("user_id", "explode(keywords) as keyword")
    user_keywords_df.selectExpr("keyword").distinct() \
        .write.format("org.neo4j.spark.DataSource") \
        .mode("overwrite") \
        .option("labels", ":Keyword") \
        .option("node.keys", "keyword") \
        .save()

    # Create INTERESTED_IN_KEYWORD relationships (User -> Keyword)
    print("Creating INTERESTED_IN_KEYWORD relationships")
    user_keywords_df.repartition(1).write.format("org.neo4j.spark.DataSource") \
        .mode("overwrite") \
        .option("relationship", "INTERESTED_IN_KEYWORD") \
        .option("relationship.save.strategy", "keys") \
        .option("relationship.source.labels", ":User") \
        .option("relationship.source.node.keys", "user_id:user_id") \
        .option("relationship.target.labels", ":Keyword") \
        .option("relationship.target.node.keys", "keyword:keyword") \
        .save()

    # Create Topic nodes (unique topics across all users)
    print("Creating Topic nodes")
    user_topics_df = agg_df.selectExpr("explode(topics) as topic").distinct()
    user_topics_df.write.format("org.neo4j.spark.DataSource") \
        .mode("overwrite") \
        .option("labels", ":Topic") \
        .option("node.keys", "topic") \
        .save()

    # Create INTERESTED_IN_TOPIC relationships (User -> Topic)
    print("Creating INTERESTED_IN_TOPIC relationships")
    user_topic_rels_df = agg_df.selectExpr("user_id", "explode(topics) as topic")
    user_topic_rels_df.repartition(1).write.format("org.neo4j.spark.DataSource") \
        .mode("overwrite") \
        .option("relationship", "INTERESTED_IN_TOPIC") \
        .option("relationship.save.strategy", "keys") \
        .option("relationship.source.labels", ":User") \
        .option("relationship.source.node.keys", "user_id:user_id") \
        .option("relationship.target.labels", ":Topic") \
        .option("relationship.target.node.keys", "topic:topic") \
        .save()
    print(" All data was loaded successfully!")

if __name__ == "__main__":
    process_data()