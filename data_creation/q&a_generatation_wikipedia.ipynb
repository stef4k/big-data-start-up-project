{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import ollama\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from google.cloud import storage\n",
    "from google.auth.exceptions import DefaultCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecf12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded wikipedia/film_pages.json to ../data/wikipedia_film_pages.json\n",
      "Downloaded wikipedia/sport_pages.json to ../data/wikipedia_sport_pages.json\n",
      "Downloaded wikipedia/tech_pages.json to ../data/wikipedia_tech_pages.json\n"
     ]
    }
   ],
   "source": [
    "# Downloading google cloud wikipedia buckets locally\n",
    "def download_from_gcs(bucket_name, blob_name, destination_file_name):\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        print(f\"Downloaded {blob_name} to {destination_file_name}\")\n",
    "    except DefaultCredentialsError as e:\n",
    "        print(\"Google Cloud credentials not found. Please set GOOGLE_APPLICATION_CREDENTIALS.\")\n",
    "        raise e\n",
    "\n",
    "BUCKET_NAME = \"group-1-landing-lets-talk\"\n",
    "blob_name = \"wikipedia/film_pages.json\"\n",
    "destination = \"../data/wikipedia_film_pages.json\"\n",
    "download_from_gcs(BUCKET_NAME, blob_name, destination)\n",
    "\n",
    "blob_name = \"wikipedia/sport_pages.json\"\n",
    "destination = \"../data/wikipedia_sport_pages.json\"\n",
    "download_from_gcs(BUCKET_NAME, blob_name, destination)\n",
    "\n",
    "blob_name = \"wikipedia/tech_pages.json\"\n",
    "destination = \"../data/wikipedia_tech_pages.json\"\n",
    "download_from_gcs(BUCKET_NAME, blob_name, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5e775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wikipedia data files\n",
    "def load_json_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e1c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint management functions\n",
    "def save_checkpoint(processed_data, output_qa_pairs, checkpoint_file='qa_generation_checkpoint.json'):\n",
    "    \"\"\"Save progress to a checkpoint file\"\"\"\n",
    "    checkpoint = {\n",
    "        'processed_data': processed_data,\n",
    "        'qa_pairs': output_qa_pairs\n",
    "    }\n",
    "    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(checkpoint, f, indent=4)\n",
    "    print(f\"Checkpoint saved: {len(processed_data)} articles processed, {len(output_qa_pairs)} QA pairs generated.\")\n",
    "\n",
    "def load_checkpoint(checkpoint_file='qa_generation_checkpoint.json'):\n",
    "    \"\"\"Load progress from a checkpoint file\"\"\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"Checkpoint loaded: {len(checkpoint['processed_data'])} articles already processed, {len(checkpoint['qa_pairs'])} QA pairs already generated.\")\n",
    "        return checkpoint['processed_data'], checkpoint['qa_pairs']\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return [], []\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628c7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_articles_in_json_files():\n",
    "    \"\"\"Count and display the number of articles in each JSON file\"\"\"\n",
    "    json_files = {\n",
    "        'Film': 'wikipedia_film_pages.json',\n",
    "        'Sport': 'wikipedia_sport_pages.json',\n",
    "        'Tech': 'wikipedia_tech_pages.json'\n",
    "    }\n",
    "    \n",
    "    print(\"===== ARTICLE COUNT IN JSON FILES =====\")\n",
    "    total_count = 0\n",
    "    \n",
    "    for category, filename in json_files.items():\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                article_count = len(data)\n",
    "                total_count += article_count\n",
    "                print(f\"{category}: {article_count} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Total articles across all files: {total_count}\")\n",
    "    print(\"========================================\")\n",
    "    \n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8394e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate Q&A pairs using Ollama with Gemma 3\n",
    "def generate_qa_pairs(article, category, num_pairs):\n",
    "    \"\"\"Generate Reddit-style Q&A pairs based on Wikipedia article content\"\"\"\n",
    "    \n",
    "    # Create a high-quality prompt for generating questions and answers\n",
    "    prompt = f\"\"\"You are an expert in {category} and a highly knowledgeable Wikipedia editor. \n",
    "    \n",
    "Based on the following Wikipedia article about \"{article['title']}\", generate {num_pairs} pairs of questions and answers that might appear on r/Ask{category.capitalize()}.\n",
    "\n",
    "The questions should:\n",
    "- Be naturally curious and conversational in tone (like genuine Reddit questions)\n",
    "- Focus on interesting facts or concepts mentioned in the article\n",
    "- Range from beginner to more advanced knowledge levels\n",
    "- Be diverse in their focus (not all about the same subtopic)\n",
    "\n",
    "The answers should:\n",
    "- Be informative and factually accurate based EXCLUSIVELY on the Wikipedia content\n",
    "- Include specific details from the article (not general knowledge)\n",
    "- Have a helpful, somewhat casual tone like a knowledgeable Reddit user\n",
    "- Be 3-5 sentences in length\n",
    "- NOT include any personal opinions or claims not supported by the article\n",
    "\n",
    "Here's the article content:\n",
    "{article['content'][:3000]}  # Limiting content length for LLM processing\n",
    "\n",
    "Format your response exactly as shown below:\n",
    "```\n",
    "Q1: [Question 1]\n",
    "A1: [Answer 1]\n",
    "\n",
    "Q2: [Question 2]\n",
    "A2: [Answer 2]\n",
    "\n",
    "Q3: [Question 3]\n",
    "A3: [Answer 3]\n",
    "```\n",
    "ONLY return the formatted Q&A pairs, no introduction or additional text.\"\"\"\n",
    "\n",
    "    # Handle rate limiting with exponential backoff\n",
    "    max_retries = 5\n",
    "    base_wait = 2\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Call Ollama with Gemma 3 model\n",
    "            response = ollama.chat(\n",
    "                model='gemma3:latest',  # Changed to gemma3:latest\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            \n",
    "            # Extract the response content\n",
    "            qa_text = response['message']['content']\n",
    "            \n",
    "            # Parse the QA pairs\n",
    "            questions = []\n",
    "            answers = []\n",
    "            \n",
    "            lines = qa_text.strip().split('\\n')\n",
    "            current_q = None\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('Q') and ':' in line:\n",
    "                    current_q = line[line.index(':')+1:].strip()\n",
    "                    questions.append(current_q)\n",
    "                elif line.startswith('A') and ':' in line and current_q:\n",
    "                    answer = line[line.index(':')+1:].strip()\n",
    "                    answers.append(answer)\n",
    "                    current_q = None\n",
    "            \n",
    "            # Return structured data\n",
    "            if len(questions) == len(answers) and len(questions) > 0:\n",
    "                return {\n",
    "                    'article_title': article['title'],\n",
    "                    'article_content': article['content'][:500] + '...',  # Store a preview of the content\n",
    "                    'category': category,\n",
    "                    'qa_pairs': [\n",
    "                        {'q': q, 'a': a} for q, a in zip(questions, answers)\n",
    "                    ]\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Warning: Mismatch in questions and answers or empty response for {article['title']}\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            wait_time = base_wait ** attempt\n",
    "            print(f\"Error: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # If all retries fail\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate a batch of QA pairs from each category\n",
    "def generate_qa_batch(num_articles_per_category=3, pairs_per_article=3, checkpoint_interval=5):\n",
    "    # Load checkpoint if exists\n",
    "    processed_article_ids, all_qa_pairs = load_checkpoint()\n",
    "    \n",
    "    # Track which articles have been processed\n",
    "    processed_ids_set = set(processed_article_ids)\n",
    "    \n",
    "    # Load data here to avoid NameError\n",
    "    film_data = load_json_data('../data/wikipedia_film_pages.json')\n",
    "    sport_data = load_json_data('../data/wikipedia_sport_pages.json')\n",
    "    tech_data = load_json_data('../data/wikipedia_tech_pages.json')\n",
    "    \n",
    "    categories = {\n",
    "        'film': film_data,\n",
    "        'sport': sport_data,\n",
    "        'tech': tech_data\n",
    "    }\n",
    "    \n",
    "    total_articles = sum(min(num_articles_per_category, len(data)) for _, data in categories.items())\n",
    "    total_processed = len(processed_ids_set)\n",
    "    \n",
    "    # Create overall progress bar\n",
    "    main_pbar = tqdm(total=total_articles, initial=total_processed, \n",
    "                     desc=\"Overall Progress\", position=0)\n",
    "    \n",
    "    # Process each category\n",
    "    try:\n",
    "        for category, data in categories.items():\n",
    "            print(f\"\\nGenerating QA pairs for {category} category...\")\n",
    "            \n",
    "            # Filter out already processed articles\n",
    "            unprocessed_data = [article for article in data \n",
    "                              if article['title'] not in processed_ids_set]\n",
    "            \n",
    "            # Select random articles from unprocessed data\n",
    "            num_to_select = min(num_articles_per_category - sum(1 for id in processed_ids_set \n",
    "                                                            if any(article['title'] == id for article in data)), \n",
    "                           len(unprocessed_data))\n",
    "            \n",
    "            if num_to_select <= 0:\n",
    "                print(f\"All requested {category} articles already processed. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            selected_articles = random.sample(unprocessed_data, num_to_select)\n",
    "            \n",
    "            # Process each article\n",
    "            for article in selected_articles:\n",
    "                print(f\"\\nProcessing: {article['title']} ({category})\")\n",
    "                \n",
    "                qa_result = generate_qa_pairs(article, category, num_pairs=pairs_per_article)\n",
    "                if qa_result:\n",
    "                    all_qa_pairs.append(qa_result)\n",
    "                \n",
    "                # Mark as processed\n",
    "                processed_ids_set.add(article['title'])\n",
    "                processed_article_ids.append(article['title'])\n",
    "                \n",
    "                # Update progress\n",
    "                main_pbar.update(1)\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if len(processed_article_ids) % checkpoint_interval == 0:\n",
    "                    save_checkpoint(processed_article_ids, all_qa_pairs)\n",
    "                \n",
    "                # Add a small delay to avoid overwhelming the API\n",
    "                time.sleep(1)\n",
    "        \n",
    "        # Final checkpoint save\n",
    "        save_checkpoint(processed_article_ids, all_qa_pairs)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user. Saving checkpoint...\")\n",
    "        save_checkpoint(processed_article_ids, all_qa_pairs)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {e}. Saving checkpoint...\")\n",
    "        save_checkpoint(processed_article_ids, all_qa_pairs)\n",
    "        raise\n",
    "    finally:\n",
    "        main_pbar.close()\n",
    "    \n",
    "    return all_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e698b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    ARTICLES_PER_CATEGORY = 500  # How many articles to process from each category\n",
    "    PAIRS_PER_ARTICLE = 2      # How many Q&A pairs to generate per article\n",
    "    CHECKPOINT_INTERVAL = 2    # Save checkpoint after processing this many articles\n",
    "    \n",
    "    # Count articles first\n",
    "    count_articles_in_json_files()\n",
    "    \n",
    "    print(\"\\nStarting Wikipedia Q&A generation process...\")\n",
    "    \n",
    "    # Generate QA pairs\n",
    "    qa_pairs = generate_qa_batch(\n",
    "        num_articles_per_category=ARTICLES_PER_CATEGORY, \n",
    "        pairs_per_article=PAIRS_PER_ARTICLE,\n",
    "        checkpoint_interval=CHECKPOINT_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # Save the QA pairs to a JSON file\n",
    "    output_file = 'wikipedia_qa_pairs.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_pairs, f, indent=4)\n",
    "    \n",
    "    total_qa_count = sum(len(article['qa_pairs']) for article in qa_pairs)\n",
    "    print(f\"\\nProcess complete!\")\n",
    "    print(f\"- Processed {len(qa_pairs)} articles\")\n",
    "    print(f\"- Generated {total_qa_count} Q&A pairs total\")\n",
    "    print(f\"- Saved to {output_file}\")\n",
    "    \n",
    "    # Print a sample QA pair from each category\n",
    "    print(\"\\n===== SAMPLE OUTPUTS =====\")\n",
    "    categories = ['film', 'sport', 'tech']\n",
    "    for category in categories:\n",
    "        category_pairs = [p for p in qa_pairs if p['category'] == category]\n",
    "        if category_pairs:\n",
    "            sample = random.choice(category_pairs)\n",
    "            print(f\"\\n--- Sample {category.upper()} Q&A ---\")\n",
    "            print(f\"Article: {sample['article_title']}\")\n",
    "            if sample['qa_pairs'] and len(sample['qa_pairs']) > 0:\n",
    "                rand_pair = random.choice(sample['qa_pairs'])\n",
    "                print(f\"Q: {rand_pair['q']}\")\n",
    "                print(f\"A: {rand_pair['a']}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking title uniqueness across datasets...\n",
      "✅ All 1500 article titles are unique across datasets.\n",
      "Loading QA pairs from wikipedia_qa_pairs.json...\n",
      "Creating backup of original file as wikipedia_qa_pairs.json.backup\n",
      "Saving updated content to wikipedia_qa_pairs_full_content.json\n",
      "✅ Updated 1500 articles with full content\n",
      "Original file backed up to: wikipedia_qa_pairs.json.backup\n",
      "Updated file saved to: wikipedia_qa_pairs_full_content.json\n"
     ]
    }
   ],
   "source": [
    "# Function to replace truncated content with full content\n",
    "def update_json_with_full_content(qa_pairs_file='wikipedia_qa_pairs.json'):\n",
    "    # First, check title uniqueness\n",
    "    print(\"Checking title uniqueness across datasets...\")\n",
    "    film_data = load_json_data('wikipedia_film_pages.json')\n",
    "    sport_data = load_json_data('wikipedia_sport_pages.json')\n",
    "    tech_data = load_json_data('wikipedia_tech_pages.json')\n",
    "    \n",
    "    # Create a title to content mapping\n",
    "    title_to_content = {}\n",
    "    \n",
    "    # Check for duplicates first\n",
    "    all_titles = []\n",
    "    for dataset in [film_data, sport_data, tech_data]:\n",
    "        for article in dataset:\n",
    "            all_titles.append(article['title'])\n",
    "    \n",
    "    unique_titles = set(all_titles)\n",
    "    if len(unique_titles) != len(all_titles):\n",
    "        print(f\" Warning: Found {len(all_titles) - len(unique_titles)} duplicate titles across datasets.\")\n",
    "        # Find and print the duplicates\n",
    "        title_counts = {}\n",
    "        for title in all_titles:\n",
    "            title_counts[title] = title_counts.get(title, 0) + 1\n",
    "        \n",
    "        duplicates = {title: count for title, count in title_counts.items() if count > 1}\n",
    "        print(f\"Duplicate titles: {', '.join(duplicates.keys())}\")\n",
    "        decision = input(\"Duplicates found. Continue anyway? (y/n): \")\n",
    "        if decision.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\" All {len(all_titles)} article titles are unique across datasets.\")\n",
    "    \n",
    "    # Build the mapping\n",
    "    for dataset in [film_data, sport_data, tech_data]:\n",
    "        for article in dataset:\n",
    "            if article['title'] not in title_to_content:  # In case of duplicates, take the first one\n",
    "                title_to_content[article['title']] = article['content']\n",
    "    \n",
    "    # Load the QA pairs file\n",
    "    print(f\"Loading QA pairs from {qa_pairs_file}...\")\n",
    "    with open(qa_pairs_file, 'r', encoding='utf-8') as f:\n",
    "        qa_pairs = json.load(f)\n",
    "    \n",
    "    # Replace truncated content with full content\n",
    "    updates = 0\n",
    "    for article in qa_pairs:\n",
    "        if article['article_title'] in title_to_content:\n",
    "            # Check if content is truncated (ends with \"...\")\n",
    "            if article['article_content'].endswith('...'):\n",
    "                article['article_content'] = title_to_content[article['article_title']]\n",
    "                updates += 1\n",
    "    \n",
    "    # Save the updated file\n",
    "    backup_file = qa_pairs_file + '.backup'\n",
    "    print(f\"Creating backup of original file as {backup_file}\")\n",
    "    import shutil\n",
    "    shutil.copy2(qa_pairs_file, backup_file)\n",
    "    \n",
    "    updated_file = qa_pairs_file.replace('.json', '_full_content.json')\n",
    "    print(f\"Saving updated content to {updated_file}\")\n",
    "    with open(updated_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_pairs, f, indent=4)\n",
    "    \n",
    "    print(f\" Updated {updates} articles with full content\")\n",
    "    print(f\"Original file backed up to: {backup_file}\")\n",
    "    print(f\"Updated file saved to: {updated_file}\")\n",
    "\n",
    "# Use this function to update your existing QA pairs JSON with full content\n",
    "update_json_with_full_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
